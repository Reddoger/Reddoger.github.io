<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Reddoge&#39;s Blog</title>
  
  <subtitle>燃尽人间琉璃色</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-03-06T09:36:07.006Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Reddoge</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>LeetCode刷题记录</title>
    <link href="http://yoursite.com/2020/03/04/LeetCode%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    <id>http://yoursite.com/2020/03/04/LeetCode%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/</id>
    <published>2020-03-04T15:23:34.000Z</published>
    <updated>2020-03-06T09:36:07.006Z</updated>
    
    <content type="html"><![CDATA[<h3 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h3><p>以前总是觉得自己代码能力很弱，但是其实本科的时候程序语言方面的课程却是我均分最高的课程，对自己否定多了导致自己开始有些不自信起来，看论文总是觉得自己原理能懂，但是要看代码的时候就是头疼。后来想练练自己的代码能力吧，但是总是学完一门语言就没有什么能用的地方。有幸在搭建博客的时候，看到各位有在博客里面写到自己刷LeetCode的过程，慢慢接触到了LeetCode，在上面零零散散做了几道题了，但是印象都不是很深刻，还是专门开一个文章来记录自己刷题的过程吧，希望两年后，真的能很自信的说自己代码能力强了。</p><h3 id="两数字之和"><a href="#两数字之和" class="headerlink" title="两数字之和"></a>两数字之和</h3><p>给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。</p><p>你可以假设每种输入只会对应一个答案。但是，你不能重复利用这个数组中同样的元素。</p><p>示例:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">给定 nums &#x3D; [2, 7, 11, 15], target &#x3D; 9</span><br><span class="line"></span><br><span class="line">因为 nums[0] + nums[1] &#x3D; 2 + 7 &#x3D; 9</span><br><span class="line"></span><br><span class="line">所以返回 [0, 1]</span><br></pre></td></tr></table></figure><h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>1、用两层循环进行解题，不过提交结果后提示 “超出时间限制”，这里就不使用了。</p><p>2、解题思路是在方法一的基础上，优化解法。想着，num2 的查找并不需要每次从 nums 查找一遍，只需要从 num1 位置之前或之后查找即可。但为了方便 index 这里选择从 num1 位置之前查找，我运行使用了28ms：</p><p>Python3：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def twoSum(nums, target):</span><br><span class="line">    lens &#x3D; len(nums)</span><br><span class="line">    j&#x3D;-1</span><br><span class="line">    for i in range(1,lens):</span><br><span class="line">        temp &#x3D; nums[:i] #从nums的地方进行查找，1st:[2,7,11,15] &#x3D;&gt; [2,7]</span><br><span class="line">        if (target - nums[i]) in temp: # 9-2 &#x3D; 7 &#x3D;&gt; True</span><br><span class="line">            j &#x3D; temp.index(target - nums[i]) # j&#x3D;1(索引)</span><br><span class="line">            break #跳出循环</span><br><span class="line">    if j&gt;&#x3D;0: #如果存在，返回[j,i]</span><br><span class="line">        return [j,i]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def twoSum(nums, target):</span><br><span class="line">    hashmap&#x3D;&#123;&#125;</span><br><span class="line">    for i,num in enumerate(nums):</span><br><span class="line">        if hashmap.get(target - num) is not None:</span><br><span class="line">            return [i,hashmap.get(target - num)]</span><br><span class="line">        hashmap[num] &#x3D; i #这句不能放在if语句之前，解决list中有重复值或target-num&#x3D;num的情况</span><br></pre></td></tr></table></figure><p>C++：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class Solution &#123;</span><br><span class="line">public:</span><br><span class="line">    vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) &#123;</span><br><span class="line">        map&lt;int,int&gt; a;&#x2F;&#x2F;提供一对一的hash</span><br><span class="line">        vector&lt;int&gt; b(2,-1);&#x2F;&#x2F;用来承载结果，初始化一个大小为2，值为-1的容器b</span><br><span class="line">        for(int i&#x3D;0;i&lt;nums.size();i++)</span><br><span class="line">        &#123;</span><br><span class="line">            if(a.count(target-nums[i])&gt;0)</span><br><span class="line">            &#123;</span><br><span class="line">                b[0]&#x3D;a[target-nums[i]];</span><br><span class="line">                b[1]&#x3D;i;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line">            a[nums[i]]&#x3D;i;&#x2F;&#x2F;反过来放入map中，用来获取结果下标</span><br><span class="line">        &#125;</span><br><span class="line">        return b;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h3&gt;&lt;p&gt;以前总是觉得自己代码能力很弱，但是其实本科的时候程序语言方面的课程却是我均分最高的课程，对自己否定多了导致自己开始有些不自信
      
    
    </summary>
    
    
      <category term="LeetCode" scheme="http://yoursite.com/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="http://yoursite.com/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>对抗样本论文阅读记录概览</title>
    <link href="http://yoursite.com/2020/03/04/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95%E6%A6%82%E8%A7%88/"/>
    <id>http://yoursite.com/2020/03/04/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95%E6%A6%82%E8%A7%88/</id>
    <published>2020-03-04T15:00:00.000Z</published>
    <updated>2020-03-31T11:22:56.342Z</updated>
    
    <content type="html"><![CDATA[<p><strong>写在前面：</strong>这篇博客用于记录在Adversarial Attack and Defence领域的论文研读，希望通过记录与思考提升自己的论文理解能力，更大的愿望是希望自己在研究生期间能成为一个优秀的科研人员，能够独立思考、解决问题、获得收获、分享经验。千里之行，始于脚下。对自己总是要有一个见证的，也是对自己的一种监督，加油！</p><img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/aisecurity-1.png" width = "900" height = "600" alt="git" align=center /><a id="more"></a><h4 id="论文列表"><a href="#论文列表" class="headerlink" title="论文列表"></a>论文列表</h4><h5 id="Boosting-Adversarial-Attacks-with-Momentum"><a href="#Boosting-Adversarial-Attacks-with-Momentum" class="headerlink" title="Boosting Adversarial Attacks with Momentum"></a>Boosting Adversarial Attacks with Momentum</h5><h6 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h6><p>这是我在谷歌学术上看到的被引数很高的一篇论文，查询了之后发现是清华大学的团队发表的文章，文章被CVPR_2018接收，研读本文目的在于了解现在最前沿的对抗攻击技术，追根溯源，加深对整个领域的认识。</p><p><img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/image-20200304210843547.png" alt="image-20200304210843547"></p><h6 id="论文阅读"><a href="#论文阅读" class="headerlink" title="论文阅读"></a>论文阅读</h6><ul><li><p>序号：1</p></li><li><p>名称：Boosting Adversarial Attacks with Momentum</p></li><li><p>年份：2018</p></li><li><p>文章类型：方法性文章</p></li><li><p>关键部分：1、攻击方法 2、攻击成功率  3、同时攻击多个模型的整体方法  4、对于局部最大值的优化方法</p></li><li><p>方法：实验/对比/优化</p></li><li><p>存在问题/引出论述：对抗性攻击是评估深度学习模型部署之前的鲁棒性的重要替代方法。但是，大多数现的对抗攻击只能成功率较低地攻击黑匣子模型。  为了解决这个问题，本文提出了一大类基于动量的迭代算法来增强对抗性攻击。通过将动量项整合到迭代的攻击过程中，本文的方法可以稳定更新方向并在迭代过程中摆脱不良的局部最大值，从而得出更具可传递性的对抗性样本。为了进一步提高黑盒攻击的成功率，还将动量迭代算法应用于一组模型，并表明经过对抗训练具有强大防御能力的的模型也容易受到我们的黑盒攻击的攻击。  </p></li><li><p>解决途径/建议/对策：1、基于动量迭代的方法，可以有效地欺骗黑盒与白盒模型 （从单个模型到一组模型）</p><p> 2、进行了广泛的实验以验证其有效性 </p><p> 3、与FGSM以及I-FGSM进行对比实验</p><p><img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/image-block1.png" alt="image-20200305195757298"></p></li><li><p>创新：1、借鉴了动量梯度下降法（Momentum）中的动量方法解决了对抗性样本容易产生的局部极大值且容易过拟合的问题。2、攻击多个模型，利用攻击的迁移性提升黑盒攻击的攻击性。3、动量方法易推广到其他的对抗攻击方法中。4、重要的参数：衰减系数μ、迭代次数、扰动大小</p></li><li><p>下一步工作/论文的不足之处：本文没有提到论文下一步的工作和不足的地方，但是通过上述表格看出，在攻击对于IncRes-v2以及Inc-V3的攻击上，效果还是不是显著，但是已经有很大的提升。下一步也是主要针对带有防御的网络，探索如何进行更加有效的攻击。</p></li><li><p>本文对自己的启发：本文通过梯度下降法中的常见方法Momentum，利用其动量特性，极大地提升了对抗性样本的攻击成功率，这一点是可以借鉴的，在以后研究对抗性样本攻击中，可以考虑一下从梯度下降法的多种方法中寻找灵感。</p></li></ul><h5 id="Explaining-And-GHarnessing-Adversarial-Examples"><a href="#Explaining-And-GHarnessing-Adversarial-Examples" class="headerlink" title="Explaining And GHarnessing Adversarial Examples"></a>Explaining And GHarnessing Adversarial Examples</h5><h6 id="前言-1"><a href="#前言-1" class="headerlink" title="前言"></a>前言</h6><p>本篇论文是提出经典对抗攻击样本生成方法FGSM的论文，也是我第一篇通过代码实现过的论文，FGSM方法对现阶段的对抗样本生成方法有着很大的影响，后面很多攻击方法也是基于FGSM方法的改进，在这里记录一下论文中的一些思路与方法，贴出Pytorch的代码供大家参考。（代码来源：<a href="https://www.cnblogs.com/tangweijqxx/p/10615950.html）" target="_blank" rel="noopener">https://www.cnblogs.com/tangweijqxx/p/10615950.html）</a></p><p><img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/image-20200308185152204.png" alt="image-20200308185152204"></p><h6 id="论文阅读-1"><a href="#论文阅读-1" class="headerlink" title="论文阅读"></a>论文阅读</h6><ul><li><p>序号：2</p></li><li><p>名称：Explaining And GHarnessing Adversarial Examples</p></li><li><p>年份：2014</p></li><li><p>文章类型：方法性文章</p></li><li><p>关键部分：1、FGSM原理：FGSM的全称是Fast Gradient Sign Method(快速梯度下降法），在白盒环境下，通过求出模型对输入的导数，然后用符号函数得到其具体的梯度方向，接着乘以一个步长，得到的“扰动”加在原来的输入上实现其攻击。</p><p> 2、攻击表达式：<br>$$<br>x’=x+\varepsilon\cdot sign(\nabla_xJ(x,y))<br>$$</p></li><li><p>方法：实验/设计/对比</p></li><li><p>存在问题/引出论述：受扰动的模型输入会使得模型以高置信度输出错误的结果，早期是以模型的非线性以及过拟合来解释这种现象（但是最近有文章提出，对于输入的扰动也是一种有用的特征信息）。本文提到神经网络容易受到对抗性扰动的主要原因在于模型的线性特性，同时这种解释也在文章中进行了首次解释。  </p></li><li><p>解决途径/建议/对策：1、对抗性样本的解释：①数字图像每个像素仅适用八位，因此会丢弃低于动态范围1/255以内的所有信息，所以如果加入小于特征精度的扰动来使分类器产生不同响应这种方法是不合理的。</p><p>②加入sign函数，使得扰动的最大范数约束增加。对于高维问题，可以通过许多无穷小的变换从而得到一个大的输出。也证明了如果简单线性模型输入具有足够的维度，就可以具有对抗性样本。</p><p>2、非线性模型扰动：①扰动造成的影响在神经网络当中会像滚雪球一样越来越大，对于线性模型越是如此。而目前神经网络中倾向于使用Relu这种类线性的激活函数，使得网络整体趋近于线性。②输入的维度越大，模型越容易受到攻击。</p></li><li><p>创新：1、提出了简单易行的攻击方法。2、利用sign函数增强扰动。3、对对抗性样本的存在性和合理性进行了解释。4、揭示了对抗性攻击的线性以及非线性对于扰动程度的影响</p></li><li><p>下一步工作/论文的不足之处：FGSM属于一步攻击，虽然刚提出时对于各种神经网络都能起到一定的攻击效果，但是随着神经网络的鲁棒性增加，以及伴随着各种防御手段的提出，攻击效果变得越来越不明显。后续基于此算法提出了例如I-FGSM等改进算法都值得学习借鉴。</p></li><li><p>本文对自己的启发：本文应当属于对抗性样本生成的最早的论文之一了，通过基于梯度的前向后向传播，探索出了一种对神经网络的攻击方式，对于整个领域来说都有着重大的意义。且本文也揭示了，对抗攻击以及防御是一个矛盾过程，会相互促进一直发展，所以要探索现阶段比较先进的攻击方式，也得多积累一些比较经典的攻击算法。</p></li></ul><h6 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">class FGSM(nn.Module):</span><br><span class="line">     def __init__(self,model):</span><br><span class="line">         super().__init__()</span><br><span class="line">         self.model&#x3D;model#必须是pytorch的model</span><br><span class="line">         self.device&#x3D;torch.device(&quot;cuda&quot; if (torch.cuda.is_available()) else &quot;cpu&quot;)</span><br><span class="line">     def generate(self,x,**params):</span><br><span class="line">         self.parse_params(**params)</span><br><span class="line">         labels&#x3D;self.y</span><br><span class="line">         if self.rand_init:</span><br><span class="line">            x_new &#x3D; x + torch.Tensor(np.random.uniform(-self.eps, self.eps, x.shape)).type_as(x).cuda()</span><br><span class="line"></span><br><span class="line">        # get the gradient of x</span><br><span class="line">        x_new&#x3D;Variable(x_new,requires_grad&#x3D;True)</span><br><span class="line">        loss_func &#x3D; nn.CrossEntropyLoss()</span><br><span class="line">        preds &#x3D; self.model(x_new)</span><br><span class="line">        if self.flag_target:</span><br><span class="line">            loss &#x3D; -loss_func(preds, labels)</span><br><span class="line">        else:</span><br><span class="line">            loss &#x3D; loss_func(preds, labels)</span><br><span class="line">        self.model.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        grad &#x3D; x_new.grad.cpu().detach().numpy()</span><br><span class="line">        # get the pertubation of an iter_eps</span><br><span class="line">        if self.ord&#x3D;&#x3D;np.inf:</span><br><span class="line">            grad &#x3D;np.sign(grad)</span><br><span class="line">        else:</span><br><span class="line">            tmp &#x3D; grad.reshape(grad.shape[0], -1)</span><br><span class="line">            norm &#x3D; 1e-12 + np.linalg.norm(tmp, ord&#x3D;self.ord, axis&#x3D;1, keepdims&#x3D;False).reshape(-1, 1, 1, 1)</span><br><span class="line">            # 选择更小的扰动</span><br><span class="line">            grad&#x3D;grad&#x2F;norm</span><br><span class="line">        pertubation &#x3D; grad*self.eps</span><br><span class="line"></span><br><span class="line">        adv_x &#x3D; x.cpu().detach().numpy() + pertubation</span><br><span class="line">        adv_x&#x3D;np.clip(adv_x,self.clip_min,self.clip_max)</span><br><span class="line"></span><br><span class="line">        return adv_x</span><br><span class="line"></span><br><span class="line">    def parse_params(self,eps&#x3D;0.3,ord&#x3D;np.inf,clip_min&#x3D;0.0,clip_max&#x3D;1.0,</span><br><span class="line">                     y&#x3D;None,rand_init&#x3D;False,flag_target&#x3D;False):</span><br><span class="line">        self.eps&#x3D;eps</span><br><span class="line">        self.ord&#x3D;ord</span><br><span class="line">        self.clip_min&#x3D;clip_min</span><br><span class="line">        self.clip_max&#x3D;clip_max</span><br><span class="line">        self.y&#x3D;y</span><br><span class="line">        self.rand_init&#x3D;rand_init</span><br><span class="line">        self.model.to(self.device)</span><br><span class="line">        self.flag_target&#x3D;flag_target</span><br></pre></td></tr></table></figure><h5 id="DeepFool-a-simple-and-accurate-method-to-fool-deep-neural-networks"><a href="#DeepFool-a-simple-and-accurate-method-to-fool-deep-neural-networks" class="headerlink" title="DeepFool: a simple and accurate method to fool deep neural networks"></a>DeepFool: a simple and accurate method to fool deep neural networks</h5><h6 id="前言-2"><a href="#前言-2" class="headerlink" title="前言"></a>前言</h6><p>DeepFool也是对抗攻击领域很出名的一种攻击算法，下图是Deepfool和FGSM添加扰动的对比，判别器都把“河马”分类为了“乌龟“，但是上图为DeepFool所添加的扰动，可见，Deepfool通过添加较少的扰动也欺骗神经网络。</p><p><img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/image-20200309193209247.png" alt="image-20200309193209247"></p><h6 id="论文阅读-2"><a href="#论文阅读-2" class="headerlink" title="论文阅读"></a>论文阅读</h6><ul><li><p>序号：3</p></li><li><p>名称：DeepFool: a simple and accurate method to fool deep neural networks  </p></li><li><p>年份：2016</p></li><li><p>文章类型：方法性文章</p></li><li><p>关键部分：1、一种简单而高效的攻击方法  2、在论文提出的期间属于可靠的攻击方法  3、利用对抗性训练提高了模型的鲁棒性  4、帮助更好地理解对抗性样本的存在</p></li><li><p>方法：实验/对比/优化</p></li><li><p>存在问题/引出论述：  深度神经网络在图像分类方面取得了巨大的成功，但同时也被证明容易受到噪声扰动的攻击，虽然这种现象对于神经网络的稳定性和安全性方面非常重要，但是至今都黑没有提出有效的方法来准确计算神经网络对于扰动的鲁棒性，本文填补了这一空白，提出了有效计算出扰动且量化分类器鲁棒性的方法。</p></li><li><p>解决途径/建议/对策：1、对于线性二分类模型，F为其分类面，f是一个分类器，最简单的最小扰动就是垂直与分类面大小为$\Delta（x_0;f）$且$r*(x_0)=argmin||r||_2$可以计算出扰动大小，其中对于距离的定义为：<br>$$<br>-f(x_0)/||w||_2^2<br>$$<br>其中，w为参数矩阵。下图为二分类模型的图像示意：</p><p><img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/image-20200309195128392.png" alt="image-20200309195128392"></p><p>2、线性二分类问题DeepFool算法流程图：</p><p><img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/image-20200310183653545.png" alt="image-20200310183653545"> </p><p>3、关于非线性多分类问题：① 首先解决多分类问题：对于多分类问题，假设$x_0$的正确分类为第四类，我们只需要分别计算$x_0$到另外三类的最短距离，然后选择最小的那个。</p><p><img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/image-20200310184003583.png" alt="image-20200310184003583"></p><p>② 对非线性问题，如下图，图中实线表示分类器真实的分类超平面，而虚线则代表近似的线性分类超平面，在每次迭代过程中，总是基于当前的迭代值，计算一组近似的线性分类超平面，并根据这组近似超平面，计算扰动，并进行迭代得到下一次的迭代值。</p><p><img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/image-20200310185433853.png" alt="image-20200310185433853"></p><p>③ 以下为非线性多分类Deepfool算法流程图：</p><p><img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/image-20200310185531677.png" alt="image-20200310185531677"></p></li><li><p>创新：1、提出了DeepFool算法，用于生成对抗样本，且添加扰动比FGSM方法要小得多。2、提出的算法能够准确地估计对抗性扰动，从而提出了一种用于估计分类器鲁棒性的方法。3、通过本文提出的方法与工具可以准确地估计最小扰动，建立鲁棒性更强的分类器。</p></li><li><p>下一步工作/论文的不足之处：本文提出了一种新的攻击方法，且添加扰动要比FGSM小，且提出了一种估计分类器鲁棒性的方法。但是同样Deepfool是针对单一样本进行噪声的添加，从而起到欺骗分类器的效果。下一步的工作可能是研究一种比较通用的扰动，也或许是针对单一扰动更加有力的对抗样本生成方法。</p></li><li><p>本文对自己的启发：本文提出的一种向量范围内的扰动距离，很值得借鉴，而且本文的一种量化思维和迭代方法也需要后来再仔细琢磨一下。这篇文章公式较多，在阅读的时候花了很多时间看公式，后续还需要自己通过代码实现来理解一下。</p></li></ul><h5 id="Universal-adversarial-perturbations"><a href="#Universal-adversarial-perturbations" class="headerlink" title="Universal adversarial perturbations"></a>Universal adversarial perturbations</h5><h6 id="前言-3"><a href="#前言-3" class="headerlink" title="前言"></a>前言</h6><p>本文在已有扰动的基础上进行了经验上的分析，提出了一种能够生成普遍性扰动的算法，并表明了这些扰动在整个神经网络中具有很高的囊括性，普遍扰动的惊人之处在于揭示了分类器的高维决策边界之间的几何关系，从而进一步揭露了输入层这单个方向上存在的安全漏洞。本文也是对抗攻击领域非常具有影响力的一篇文章，</p><p><img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/image-20200316190634628.png" alt="image-20200316190634628"></p><h6 id="论文阅读-3"><a href="#论文阅读-3" class="headerlink" title="论文阅读"></a>论文阅读</h6><ul><li><p>序号：4</p></li><li><p>名称：Universal adversarial perturbations  </p></li><li><p>年份：2017</p></li><li><p>文章类型：方法性文章</p></li><li><p>关键部分：1、提出了一种构造不可察觉的普遍性扰动的方法  2、普遍性扰动对于最先进的深度神经网络有很好的干扰作用  3、证明了普遍性扰动的泛化特征  4、通过实验解释和分析了神经网络的高维脆弱性 5、确定了决策边界的不同部分的几何相关性</p></li><li><p>方法：实验/对比/优化</p></li><li><p>存在问题/引出论述：  在之前两篇论文笔记中提及的对抗样本生成方法几乎都是对不同的对抗样本添加不同的噪声，噪声依赖于某一特定样本的特征。本文中，作者试图寻找一种通用的对抗扰动，使添加完该扰动后的所有原始图片都会被误分类为其他类别。</p></li><li><p>解决途径/建议/对策：1、本文所寻找的对抗扰动在保证不影响原始样本的真实分布的前提下，具有很好的泛化能力，不仅是所有输入图片通用，还在不同的网络结构间通用，算法流程如下：</p><img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/image-20200316215034856.png" alt="image-20200316215034856" style="zoom: 80%;" /><p>2、算法解释：</p></li></ul><p>  令$μ$ 为一组图片的分布，$\widetilde k$ 表示分类器，本文的目的是要寻找到一个$v$，使得大多数分布为$μ$ 的图片，在添加完该噪声之后被分类器分类为与真实类别不同的类别（无目标攻击），即：</p><p>$$<br>  \widetilde k(x+v)\neq \widetilde k(x)<br>$$<br>  且所求$v$扰动需满足：</p><p>  ① 扰动大小约束：$||v||\leq \xi$</p><p>  ② 扰动成功率约束条件：$P_x(\widetilde k(x+v)\neq \widetilde k(x)) \geq 1-\delta$</p><p>  3、扰动的更新过程：</p>  <img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/image-20200316220827236-1584367733692.png" style="zoom:80%;" /><p>  如上图，$x_1$,$x_2$,$x_3$样本点是重合的，$R_1$,$R_2$,$R_3$表示不同的分类域。先在$R_1$中，找到最佳的脱离分类域的扰动，更新一次$x$，再根据新的$x$找到脱离$R_2$分类域的扰动，以此类推，最终计算出最优扰动$v$。</p><p>  具体的计算方法 如下：</p><p>  ① 先令$v=0$，如果当前扰动不是最优的扰动，则：$\Delta v_i \leftarrow argmin||r||_2$  使得 $\widetilde k(x_i+v+r)\neq \widetilde k(x_i)$。</p><p>  ② 要使得$v$满足约束条件，对更新的扰动进行变换：$P_x,_\xi(v)=argmin_v||v’-v||$ s.t.$||v’||_p\leq \xi$</p><p>  ③ $v$的更新法则：$\Delta v \leftarrow P_x,_\xi(v+\Delta v_i)$</p><p>  ④ 算法的截止条件为：$Err(X_v):=\frac{1}{m}\sum_{i=1}^m1_{\widetilde k(x+v)\neq \widetilde k(x)}\geq 1-\delta$</p><p>  其中，$X_v :=${$x_1+v,\cdots,x_m+v$}标示原数据集添加了同一扰动后生成的对抗样本集，上式计算了所得扰动的成功率，当成功率大于预设值时，训练结束，算法流程如下图所示。</p><ul><li><p>创新：1、提出了一种通用的扰动，且在多种先进网络上取得了很好的成功率。</p><p><img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/image-20200316223844506.png" alt="image-20200316223844506"></p><p>2、在不同模型之间的白盒以及黑盒攻击进行实验，得到了如下图的成功率。</p><p><img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/image-20200316224020635.png" alt="image-20200316224020635"></p><p>3、通过实验解释和分析了神经网络的高维脆弱性以及决策边界的不同部分的几何相关性。</p><img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/image-20200316224313604.png" alt="image-20200316224313604" style="zoom:80%;" /></li><li><p>下一步工作/论文的不足之处：1、更加强力的通用扰动算法 2、针对目前存在的防御方式的通用扰动方式设计 3、对决策边界不同部分之间的几何相关性的理论分析将成为未来研究的主题。  </p></li><li><p>本文对自己的启发：本文提出了一种可以用于大多数神经网络的普遍性扰动的生成算法，且解释了神经网络的高维脆弱性以及决策边界的相关性，是一篇很有内容且很有启发的文章。也告诉研究者，要通过单一的扰动现象去寻找更深层次的规律和特征。近期也有一篇文章指出，对抗攻击样本并不是神经网络的bug而是确实有用的特征信息。所以针对对抗性样本的普遍规律的寻找和神经网络高维脆弱性的探索，也是后续的一些很有意义的工作。</p></li></ul><h5 id="FDA-Feature-Disruptive-Attack"><a href="#FDA-Feature-Disruptive-Attack" class="headerlink" title="FDA: Feature Disruptive Attack"></a>FDA: Feature Disruptive Attack</h5><h6 id="前言-4"><a href="#前言-4" class="headerlink" title="前言"></a>前言</h6><p>以往攻击方法大多通过与网络的pre-softmax或softmax相关联的优化目标（<strong>标签损失</strong>）生成对抗扰动。这一篇论文展示了这种对抗攻击方式的缺点。且提出了两个新的评估指标：① 旧标签新登记 ② 新标签旧等级。并提出了一种新的攻击方式：<strong>特征攻击</strong>，来解决现阶段攻击存在的问题。</p><img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/image-20200319174535066.png" alt="image-20200319174535066" style="zoom:80%;" /><h6 id="论文阅读-4"><a href="#论文阅读-4" class="headerlink" title="论文阅读"></a>论文阅读</h6><ul><li><p>序号：5</p></li><li><p>名称：FDA: Feature Disruptive Attack  </p></li><li><p>年份：2019</p></li><li><p>文章类型：方法性文章</p></li><li><p>关键部分：1、证明了现有攻击方法的弱点。 2、提出了两个新的评估指标，即NLOR和OLNR，以量化攻击方法造成的扰动程度。  3、引入了一种新的攻击，称为FDA，其动机是破坏每一层的功能。  4、成功地攻击了两个基于特征的任务 </p></li><li><p>方法：实验/对比/优化</p></li><li><p>存在问题/引出论述：  具有不可察觉的噪声的图像样本经过设计可操纵网络的预测结果。对抗性样本生成方法的范围从简单到复杂，大多都会通过与网络的softmax或softmax层来输出相关的优化目标来生成对抗样本，但这些方法存在一些弊端，如使用此类攻击的对抗样本中依然会保留干净样本的高级语义信息且依然可以用于各种功能性驱动任务，如风格转换 。</p></li><li><p>解决途径/建议/对策：1、新的评估指标(NLOR and OLNR)：本文提出了两个新的评估指标：新La-bel旧等级（NLOR）和旧标签新等级（OLNR）。对于给定的输入图像， 分类器的softmax输出表示每个类别的置信度。本文将这些类别的置信度按降序排列（从等级1到C）。将攻击之前的网络预测作为旧标签，并将攻击之后的网络预测作为新标签。攻击后，旧标签的等级将从1变为“ p”。旧标签的新等级“ p”定义为OLNR（旧标签的新等级）。此外，在攻击后，新标签的等级将从“ q”更改为1。新标签的旧等级“ q”定义为NLOR（新标签的旧等级）。因此，较强的攻击应该跳到具有较高旧等级（将产生较高的NLOR）的标签，并且还应减少进行纯预测的概率（将产生较高的OLNR）。针对所有错误分类的图像计算这些指标，并计算得出平均值。  </p><p>2、算法解释：</p></li></ul><p>  <img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/image-20200331171902107.png" alt="image-20200331171902107"></p><p>  简单来说，就是得到一张干净图片的激活值（有的大有的小），通过优化方法，让对抗图片得到的激活值尽可能反着来——大的减小，小的增大。</p><p>  具体的计算公式如下：</p>  <img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/image-20200331172524030.png" alt="image-20200331172524030" style="zoom:80%;" /><p>  3、结果：</p>  <img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/image-20200331172627656.png" alt="image-20200331172627656" style="zoom: 80%;" /><ul><li><p>创新：1、在对抗攻击过程中，发现了关于对抗样本的纯净特征的保存，且对抗攻击大多都与softmax有关。</p><p>2、建议使用评估指标（即OLNR以及NLOR）作为对抗攻击效果的评估标准。</p><p>3、提出了FDA对抗性攻击。通过验证是当时最强大的白盒模型攻击方法之一。</p><p>4、验证了FDA攻击后特征不会被用于风格迁移和字幕生成等工作，进一步验证了攻击的强大。</p></li></ul><ul><li><p>下一步工作/论文的不足之处：1、针对于攻击方法，是否可以还有更好的不使用Softmax层的攻击方法。 2、对于激活值的梯度迭代中更加快速的算法。 </p></li><li><p>本文对自己的启发：本文针对目前存在的对抗性样本生成方法，提出了普遍性的不足，即大多对抗算法都会通过与网络的softmax或softmax层来输出相关的优化目标来生成对抗样本，针对此问题，作者提出了一种FDA（即特征破坏攻击）旨在于针对每一层的功能进行攻击破坏来起到对抗性样本的生成目的，在此基础上还提出了更加有效的关于对抗攻击的评估指标。本文对我一个比较大的启发是，在之前，大部分的对抗攻击算法都是针对于输入和输出层进行梯度设计，并生成对抗性样本的，而本文则是针对每一层的功能进行破坏，从整体上破坏了神经网络的功能，故产生了很好的效果。所以对于对抗攻击，下一步更加有效的攻击方法，猜想可能是对于对抗样本特征的更深层的剖析，或者对于神经网络每一层具体功能进行对抗设计，从而产生更加有效的攻击样本。</p></li></ul><h5 id="Advhat：Real-World-Adversarial-attack-on-Arcface-face-ID-System"><a href="#Advhat：Real-World-Adversarial-attack-on-Arcface-face-ID-System" class="headerlink" title="Advhat：Real-World Adversarial attack on Arcface face ID System"></a>Advhat：Real-World Adversarial attack on Arcface face ID System</h5><h6 id="前言-5"><a href="#前言-5" class="headerlink" title="前言"></a>前言</h6><p>本文提出了一种易于操作和复制的新技术，这种技术可以使得攻击者在不同的拍摄条件下都能攻击识别性能很好的公开人脸识别系统ArcFace。而进行攻击的方式则是在普通彩色打印机上打印了一张矩形贴纸并将其戴在帽子上进行攻击。对抗性贴纸采用一种新的算法进行生成，用于对图像进行平面外转换。这种方法会混淆最新的公共 Face ID 模 型 LResNet100E-IR ， ArcFace并且可以转移到其他FaceID模型。  </p><img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/image-20200331183649804.png" alt="image-20200331183649804" style="zoom:80%;" /><h6 id="论文阅读-5"><a href="#论文阅读-5" class="headerlink" title="论文阅读"></a>论文阅读</h6><ul><li><p>序号：5</p></li><li><p>名称：Advhat：Real-World Adversarial attack on Arcface face ID System</p></li><li><p>年份：2019</p></li><li><p>文章类型：应用性文章</p></li><li><p>关键部分：1、提出了一种使用帽子贴纸的针对Face-ID的攻击方式  2、攻击易于重现，只需要进行彩色矩阵的打印即可  3、攻击在不同的拍摄角度下依旧可以起到作用  4、提出了一种新的方法使得攻击过程中贴纸会投影到图像中，使其变得真实。5、攻击可以迁移到其他的Face-ID系统。</p></li><li><p>方法：实验/对比/优化</p></li><li><p>存在问题/引出论述：  </p></li><li><p>解决途径/建议/对策：1、有关于物理世界中的攻击：尽管对抗攻击在数字领域很成功，但是在真实世界中，对抗攻击的效率依旧很低，但是也具有进一步发展的潜力，如将对抗性图像印在手机上然后使用手机拍摄，仍然可以欺骗神经网络。</p><p>2、关于构造现实世界对抗性示例最成功的典范——期望转换（EOT）算法：</p><p>​                                                              $1.f(x)=c_{gt}\neq c_t$</p><img src="E:\web\GitHub\Reddoge\source\image\image-20200331190001739.png" alt="image-20200331190001739" style="zoom: 67%;" /><p>3、贴纸的转换：作者将贴纸放置时发生的转换分为两个步骤，贴纸的平面外弯曲和贴纸的俯仰旋转，作者将这两种变化都模拟为3D空间中的抛物线变换，并使用空间变换层（STL）将获得的标签投影到面部图像上。</p><p>4、攻击方式：本文中使用具有动量的迭代FGSM和在实验中有效的几种启发式方法。将攻击分为两个阶段。① 在第一阶段，只用等于于5的步长值和等于0.9的动量。②在第二阶段，我们使用等于255的步长值，动量等于0.995。损失的权重始终等于1e-4。使用一张带有标签的固定图像作为验证，在该验证中，将所有参数设置为最真实的外观值。最后使用最小二乘法通过线性函数对最后100个验证值进行插值：第一阶段进行100次迭代之后，第二阶段进行200次迭代之后。如果该线性函数的角系数不小于零，则：如果在第一阶段，进入攻击的第二阶段；如果在第二阶段，则停止攻击。  </p></li></ul><p>  5、实验的几种情况：① 在固定条件下进行实验 ② 在各种不同的条件下进行训练 ③ 进行迁移性实验</p><p>  下图为十一张不同的照片，以便于检查各种情况下的攻击效果：</p>  <img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/image-20200331191212094.png" alt="image-20200331191212094" style="zoom:80%;" /><p>  下图为各种拍摄条件下基准线和最终的相似度：</p>  <img src="http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/image-20200331191312545.png" alt="image-20200331191312545" style="zoom:80%;" /><ul><li><p>创新：1、提出了一种攻击人脸ID系统的新方法，称为AdvHat。  </p><p>2、此方法可以轻松重现，并且可以在不同的拍摄条件下有效地攻击最佳的公开人脸ID模型。  </p><p>3、实验结果证明了对最新的Face ID系统ArcFace攻击的鲁棒性。</p></li></ul><ul><li><p>下一步工作/论文的不足之处：1、有希望将模型应用于最新的人脸检测器。   2、对于真实世界的对抗性样本攻击成功率较低的问题还需解决  3、对于真实世界的对抗性样本的高维特征分析依旧是一个比较深入的问题</p></li><li><p>本文对自己的启发：本文提出了一种物理世界的对抗性样本，成为AdvHat，佩戴于帽子上可以成功欺骗最新的Face ID系统，这种攻击方式最大的有点就是操作简单，且对Face ID系统的威胁较大，如果后续能够成功提出定向的对抗攻击样本，则对于现在Face ID 的安全性将会是巨大的威胁。本文对于我还有一个较大的启发就是通过经典的对抗攻击样本算法，可以衍生出很多相关应用，很多研究点还待发掘。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;写在前面：&lt;/strong&gt;这篇博客用于记录在Adversarial Attack and Defence领域的论文研读，希望通过记录与思考提升自己的论文理解能力，更大的愿望是希望自己在研究生期间能成为一个优秀的科研人员，能够独立思考、解决问题、获得收获、分享经验。千里之行，始于脚下。对自己总是要有一个见证的，也是对自己的一种监督，加油！&lt;/p&gt;
&lt;img src=&quot;http://q6kao6qhw.bkt.clouddn.com/qiniu_picGO/aisecurity-1.png&quot; width = &quot;900&quot; height = &quot;600&quot; alt=&quot;git&quot; align=center /&gt;
    
    </summary>
    
    
      <category term="Adversarial Attack" scheme="http://yoursite.com/categories/Adversarial-Attack/"/>
    
    
      <category term="Adversarial Attack" scheme="http://yoursite.com/tags/Adversarial-Attack/"/>
    
  </entry>
  
  <entry>
    <title>诗酒趁年华</title>
    <link href="http://yoursite.com/2020/02/28/%E8%AF%97%E9%85%92%E8%B6%81%E5%B9%B4%E5%8D%8E/"/>
    <id>http://yoursite.com/2020/02/28/%E8%AF%97%E9%85%92%E8%B6%81%E5%B9%B4%E5%8D%8E/</id>
    <published>2020-02-28T15:23:34.000Z</published>
    <updated>2020-03-06T09:34:11.337Z</updated>
    
    <content type="html"><![CDATA[<h5 id="欢迎来到我的第一篇博客。"><a href="#欢迎来到我的第一篇博客。" class="headerlink" title="欢迎来到我的第一篇博客。"></a>欢迎来到我的第一篇博客。</h5>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h5 id=&quot;欢迎来到我的第一篇博客。&quot;&gt;&lt;a href=&quot;#欢迎来到我的第一篇博客。&quot; class=&quot;headerlink&quot; title=&quot;欢迎来到我的第一篇博客。&quot;&gt;&lt;/a&gt;欢迎来到我的第一篇博客。&lt;/h5&gt;
      
    
    </summary>
    
    
      <category term="Chatting" scheme="http://yoursite.com/categories/Chatting/"/>
    
    
      <category term="Chatting" scheme="http://yoursite.com/tags/Chatting/"/>
    
  </entry>
  
</feed>
